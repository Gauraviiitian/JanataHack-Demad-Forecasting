{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gaurav misra\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = test_data.record_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one data point is nan so we drop it without hesitating\n",
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The day of the week with Monday=0, Sunday=6\n",
    "train_data.week = pd.to_datetime(train_data.week)\n",
    "train_data['dow'] = train_data['week'].dt.dayofweek\n",
    "\n",
    "test_data.week = pd.to_datetime(test_data.week)\n",
    "test_data['dow'] = test_data['week'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['is_bp_eq_tp'] = train_data.base_price-train_data.total_price\n",
    "test_data['is_bp_eq_tp'] = test_data.base_price-test_data.total_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['record_ID', 'week']\n",
    "train_data = train_data.drop(cols_to_drop, axis=1)\n",
    "test_data = test_data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data.units_sold\n",
    "X = train_data.drop(['units_sold'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_sku = pd.get_dummies(X.sku_id)\n",
    "ohe_store = pd.get_dummies(X.store_id)\n",
    "\n",
    "cols_store = list(ohe_store.columns)\n",
    "for each in cols_store:\n",
    "    X[each] = ohe_store[each]\n",
    "\n",
    "cols_sku = list(ohe_sku.columns)\n",
    "for each in cols_sku:\n",
    "    X[each] = ohe_sku[each]\n",
    "\n",
    "X = X.drop(['sku_id', 'store_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this test data preprocessing required only for xgboost\n",
    "ohe_sku = pd.get_dummies(test_data.sku_id, drop_first=True)\n",
    "ohe_store = pd.get_dummies(test_data.store_id, drop_first=True)\n",
    "\n",
    "cols_store = list(ohe_store.columns)\n",
    "for each in cols_store:\n",
    "    test_data[each] = ohe_store[each]\n",
    "\n",
    "cols_sku = list(ohe_sku.columns)\n",
    "for each in cols_sku:\n",
    "    test_data[each] = ohe_sku[each]\n",
    "\n",
    "test_data = test_data.drop(['sku_id', 'store_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13860, 109)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_col = list(test_data.columns)\n",
    "train_col = list(X.columns)\n",
    "\n",
    "for each in train_col:\n",
    "    if each not in test_col:\n",
    "        test_data[each] = [0]*test_data.shape[0]\n",
    "\n",
    "test_data = test_data[train_col]\n",
    "\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150149, 109)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# initialize the column names of the continuous data\n",
    "continuous = ['total_price', 'base_price', 'is_bp_eq_tp']\n",
    "\n",
    "# performin min-max scaling each continuous feature column to\n",
    "# the range [0, 1]\n",
    "scaler.fit(X[continuous])\n",
    "X[continuous] = scaler.transform(X[continuous])\n",
    "test_data[continuous] = scaler.transform(test_data[continuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_price</th>\n",
       "      <th>base_price</th>\n",
       "      <th>is_featured_sku</th>\n",
       "      <th>is_display_sku</th>\n",
       "      <th>is_bp_eq_tp</th>\n",
       "      <th>8023</th>\n",
       "      <th>8058</th>\n",
       "      <th>8063</th>\n",
       "      <th>8091</th>\n",
       "      <th>8094</th>\n",
       "      <th>...</th>\n",
       "      <th>320485</th>\n",
       "      <th>327492</th>\n",
       "      <th>378934</th>\n",
       "      <th>398721</th>\n",
       "      <th>545621</th>\n",
       "      <th>546789</th>\n",
       "      <th>547934</th>\n",
       "      <th>600934</th>\n",
       "      <th>673209</th>\n",
       "      <th>679023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.280438</td>\n",
       "      <td>0.251778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.822161</td>\n",
       "      <td>0.815078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.548564</td>\n",
       "      <td>0.530583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.331437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.331437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_price  base_price  is_featured_sku  is_display_sku  is_bp_eq_tp  \\\n",
       "0     0.280438    0.251778                0               0     0.298805   \n",
       "1     0.822161    0.815078                0               0     0.298805   \n",
       "2     0.548564    0.530583                0               0     0.298805   \n",
       "3     0.357045    0.331437                0               0     0.298805   \n",
       "4     0.357045    0.331437                0               0     0.298805   \n",
       "\n",
       "   8023  8058  8063  8091  8094  ...  320485  327492  378934  398721  545621  \\\n",
       "0     0     0     0     0     0  ...       0       0       0       0       0   \n",
       "1     0     0     0     0     0  ...       0       0       0       0       0   \n",
       "2     0     0     0     0     0  ...       0       0       0       0       0   \n",
       "3     0     0     0     0     0  ...       0       0       0       0       0   \n",
       "4     0     0     0     0     0  ...       0       0       0       0       0   \n",
       "\n",
       "   546789  547934  600934  673209  679023  \n",
       "0       0       0       0       0       0  \n",
       "1       0       0       0       0       0  \n",
       "2       0       0       0       0       0  \n",
       "3       0       0       0       0       0  \n",
       "4       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# create model\n",
    "model = Sequential([\n",
    "    Dense(input_dim=train_X.shape[1], activation='relu', units=128),\n",
    "    Dense(input_dim=128, activation='relu', units=64),\n",
    "    Dense(input_dim=64, activation='relu', units=32),\n",
    "    Dense(input_dim=32, units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with necessary attributes\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    # metrics=['mse'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "checkpoint = ModelCheckpoint('model_best_weight.h5', monitor='val_loss', save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 120119 samples, validate on 30030 samples\n",
      "Epoch 1/25\n",
      "120119/120119 [==============================] - 15s 124us/step - loss: 1317.9679 - val_loss: 954.5389\n",
      "Epoch 2/25\n",
      "120119/120119 [==============================] - 14s 113us/step - loss: 867.3263 - val_loss: 882.0563\n",
      "Epoch 3/25\n",
      "120119/120119 [==============================] - 14s 113us/step - loss: 812.7767 - val_loss: 859.3296\n",
      "Epoch 4/25\n",
      "120119/120119 [==============================] - 14s 114us/step - loss: 781.2877 - val_loss: 839.6828\n",
      "Epoch 5/25\n",
      "120119/120119 [==============================] - 13s 112us/step - loss: 760.1718 - val_loss: 873.5232\n",
      "Epoch 6/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 745.7337 - val_loss: 1003.4966\n",
      "Epoch 7/25\n",
      "120119/120119 [==============================] - 14s 114us/step - loss: 734.9571 - val_loss: 872.1407\n",
      "Epoch 8/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 730.9195 - val_loss: 865.4874\n",
      "Epoch 9/25\n",
      "120119/120119 [==============================] - 14s 113us/step - loss: 720.8817 - val_loss: 846.9763\n",
      "Epoch 10/25\n",
      "120119/120119 [==============================] - 15s 126us/step - loss: 716.6989 - val_loss: 856.4103\n",
      "Epoch 11/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 706.8782 - val_loss: 864.2759\n",
      "Epoch 12/25\n",
      "120119/120119 [==============================] - 14s 114us/step - loss: 702.1593 - val_loss: 827.6121\n",
      "Epoch 13/25\n",
      "120119/120119 [==============================] - 14s 113us/step - loss: 696.5928 - val_loss: 850.3834\n",
      "Epoch 14/25\n",
      "120119/120119 [==============================] - 14s 118us/step - loss: 690.9586 - val_loss: 846.4491\n",
      "Epoch 15/25\n",
      "120119/120119 [==============================] - 15s 122us/step - loss: 686.7575 - val_loss: 845.5410\n",
      "Epoch 16/25\n",
      "120119/120119 [==============================] - 16s 129us/step - loss: 691.6567 - val_loss: 851.3249\n",
      "Epoch 17/25\n",
      "120119/120119 [==============================] - 16s 129us/step - loss: 676.6029 - val_loss: 842.3872\n",
      "Epoch 18/25\n",
      "120119/120119 [==============================] - 15s 126us/step - loss: 671.7553 - val_loss: 895.5941\n",
      "Epoch 19/25\n",
      "120119/120119 [==============================] - 15s 121us/step - loss: 670.0627 - val_loss: 865.9322\n",
      "Epoch 20/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 670.4817 - val_loss: 851.7864\n",
      "Epoch 21/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 659.9823 - val_loss: 855.0179\n",
      "Epoch 22/25\n",
      "120119/120119 [==============================] - 14s 115us/step - loss: 663.7837 - val_loss: 855.8564\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    batch_size=32,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(val_X, val_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained best model weights into new model\n",
    "model.load_weights(\"model_best_weight.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (30030, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions (the output of the last layer)\n",
    "# on new data using `predict`\n",
    "predictions = model.predict(val_X)\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18084016248031187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "msle = mean_squared_log_error(val_y, predictions)\n",
    "# mse = mean_squared_error(val_y, predictions)\n",
    "print(msle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_data)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['record_ID'] = rid\n",
    "subm['units_sold'] = preds\n",
    "\n",
    "subm.to_csv(\"submission_deep1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
